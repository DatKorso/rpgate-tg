# Sprint 3 Specification (UPDATED): Cost-Optimized Memory System

> **–û–±–Ω–æ–≤–ª–µ–Ω–æ:** 7 –Ω–æ—è–±—Ä—è 2025 –≥.  
> **–ò–∑–º–µ–Ω–µ–Ω–∏—è:** –£—á—Ç–µ–Ω—ã —Ä–µ—à–µ–Ω–∏—è –ø–æ 3 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–æ–±–ª–µ–º–∞–º - LLM-based importance scoring, temporal ranking –±–µ–∑ session summaries, confidence-based knowledge scoping

---

## üìã Sprint Overview

**–¶–µ–ª—å:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å (RAG) —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∑–∞—Ç—Ä–∞—Ç, –¥–æ–±–∞–≤–∏—Ç—å persistence —á–µ—Ä–µ–∑ Supabase, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å Memory Manager –∏ World State –∞–≥–µ–Ω—Ç–æ–≤.

**Timeframe:** 2-3 –Ω–µ–¥–µ–ª–∏

**Success Criteria:**
- ‚úÖ –ë–æ—Ç –ø–æ–º–Ω–∏—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ –ø—Ä–æ—à–ª—ã—Ö —Å–µ—Å—Å–∏–π (multi-session continuity)
- ‚úÖ Memory retrieval —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ (<500ms) –∏ —Ç–æ—á–Ω–æ (>85% accuracy)
- ‚úÖ World State Agent –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç game state
- ‚úÖ Memory Manager Agent –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- ‚úÖ –î–∞–Ω–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ PostgreSQL
- ‚úÖ **LLM-based importance scoring** (zero overhead —á–µ—Ä–µ–∑ Synthesizer)
- ‚úÖ **Confidence-based knowledge scoping** (–º–µ—Ç–∞–≥–µ–π–º–∏–Ω–≥ prevention)
- ‚úÖ **Temporal ranking** –¥–ª—è –±–µ—Å—à–æ–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π (no session summaries)
- ‚úÖ **Cost per turn <$0.0025** (5% overhead –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ)

---

## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è Sprint 3

### –î–æ Sprint 3 (—Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ):
```
User Input ‚Üí Rules Arbiter ‚Üí Narrative Director ‚Üí Response Synthesizer
                ‚Üì                   ‚Üì                      ‚Üì
            Character         Game State            Final Message
             (FSM)             (FSM)                (Telegram)
```

### –ü–æ—Å–ª–µ Sprint 3 (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞):
```
User Input
    ‚Üì
Memory Manager Agent (Layered retrieval: recent + important + semantic)
    ‚Üì
[Simple Sequential Orchestration]
    ‚Üì
Rules Arbiter ‚îÄ‚îÄ‚îê
                ‚îú‚îÄ‚Üí World State Agent ‚Üí Update DB
Narrative ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚Üì
                Response Synthesizer (+ metadata extraction –≤ JSON)
                        ‚Üì
                  Final Message
                        ‚Üì
                  Smart Memory Storage (importance + confidence filtering)
```

**–ö–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
1. **Layered Memory Retrieval** - recent + important + semantic –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ
2. **Zero-overhead metadata** - Synthesizer –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç importance/confidence –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º JSON
3. **No session summaries** - temporal ranking –≤–º–µ—Å—Ç–æ —è–≤–Ω—ã—Ö session boundaries
4. **Confidence-based scoping** - 3-level system (1.0, 0.5, 0.0) –¥–ª—è –º–µ—Ç–∞–≥–µ–π–º–∏–Ω–≥ prevention
5. **Simple orchestrator** - –±–µ–∑ CrewAI –¥–ª—è MVP (–ø—Ä–æ—â–µ, –¥–µ—à–µ–≤–ª–µ, –±—ã—Å—Ç—Ä–µ–µ)

---

## üîë –†–µ—à–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º

### Problem 1: Importance Scoring (—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫)

**‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º:** Keyword heuristics (–Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏–∑-–∑–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏)

**‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º:** LLM-based scoring —á–µ—Ä–µ–∑ Response Synthesizer (zero overhead)

**Implementation:**

```python
# –í Response Synthesizer prompt –¥–æ–±–∞–≤–ª—è–µ–º:

system_prompt = """...(existing prompt)...

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤–µ—Ä–Ω–∏ JSON —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏:
{
  "final_message": "...",
  "memory_metadata": {
    "importance_score": 0-10,  // 0-2: trivial, 3-5: normal, 6-8: important, 9-10: critical
    "player_knowledge_confidence": 0.0-1.0,  // 1.0: knows, 0.5: unclear, 0.0: GM secret
    "key_entities": ["NPC names", "locations"],
    "memory_type": "event|dialogue|discovery|combat"
  }
}

–ö—Ä–∏—Ç–µ—Ä–∏–∏ importance:
- 9-10: Boss fights, plot twists, character death, major discoveries
- 6-8: Quest –ø–æ–ª—É—á–µ–Ω–∏–µ/completion, NPC betrayals, significant loot
- 3-5: Normal combat, dialogue, movement
- 0-2: Trivial actions, simple observations
"""
```

**Cost:** $0.00 (—É–∂–µ –≤ –±–∞–∑–æ–≤–æ–º Synthesizer call)

---

### Problem 2: Session Summaries (–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å)

**‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º:** Session summaries (–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ + extra LLM calls)

**‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º:** Layered retrieval + temporal ranking

**Implementation:**

```python
async def get_context_for_turn(character_id: UUID, query: str) -> str:
    """
    Single-pass memory retrieval –±–µ–∑ session summaries.
    
    Layers:
    1. Recent (last 10 memories) - –¥–ª—è immediate continuity
    2. Important (importance >= 7) - –¥–ª—è key plot points
    3. Semantic (vector search) - –¥–ª—è specific context
    """
    
    # Layer 1: Recent (cheap SQL, no vectors)
    recent = await episodic_memory.get_recent_memories(
        character_id=character_id,
        limit=10
    )
    
    # Layer 2+3: Important + Semantic combined (one vector search)
    relevant = await episodic_memory.search_with_temporal_ranking(
        character_id=character_id,
        query_text=query,
        top_k=5,
        min_importance=6,
        recency_weight=0.3  # 70% semantic similarity, 30% recency
    )
    
    # Deduplicate by ID
    all_memories = deduplicate_by_id(recent + relevant)
    
    # Build context string
    return build_memory_context(all_memories)
```

**Temporal Ranking Formula:**

```python
# –í search_with_temporal_ranking():

final_score = (
    semantic_similarity * 0.7 +
    recency_score * 0.3
)

# recency_score = 1.0 –¥–ª—è today, 0.0 –¥–ª—è 30+ days ago
recency_score = max(0, 1 - (days_since_creation / 30))
```

**Benefits:**
- –°—Ç–∞—Ä—ã–µ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (importance=9) –≤—Å–ø–ª—ã–≤–∞—é—Ç —á–µ—Ä–µ–∑ high importance
- –ù–µ–¥–∞–≤–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è (importance=5) –≤—Å–ø–ª—ã–≤–∞—é—Ç —á–µ—Ä–µ–∑ recency
- –ù–µ—Ç LLM calls –¥–ª—è summaries
- –ë–µ—Å—à–æ–≤–Ω–∞—è –∏–≥—Ä–∞ - –Ω–µ—Ç —è–≤–Ω—ã—Ö session boundaries

---

### Problem 3: Knowledge Scoping (–º–µ—Ç–∞–≥–µ–π–º–∏–Ω–≥)

**‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º:** Hard metadata tagging (—Ö—Ä—É–ø–∫–æ, –æ—à–∏–±–∫–∏ LLM)

**‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º:** Probabilistic confidence scores (3 levels)

**Implementation:**

```python
# –í Response Synthesizer instructions:

"""
player_knowledge_confidence:
- 1.0: –ü–µ—Ä—Å–æ–Ω–∞–∂ —Ç–æ—á–Ω–æ –∑–Ω–∞–µ—Ç (observed directly, told by NPC)
- 0.5: –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ (rumor, inference, –≤–µ—Ä–æ—è—Ç–Ω–æ –∑–Ω–∞–µ—Ç)
- 0.0: GM secret (–ø–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ –º–æ–∂–µ—Ç –∑–Ω–∞—Ç—å)

–ü—Ä–∏–º–µ—Ä—ã:
- "–ò–≥—Ä–æ–∫ –≤—Å—Ç—Ä–µ—Ç–∏–ª —Ç–æ—Ä–≥–æ–≤—Ü–∞" ‚Üí 1.0 (–∑–Ω–∞–µ—Ç)
- "–¢–æ—Ä–≥–æ–≤–µ—Ü –≤—ã–≥–ª—è–¥–∏—Ç –Ω–µ—Ä–≤–Ω—ã–º" ‚Üí 1.0 (observed)
- "–í –ø–µ—â–µ—Ä–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø–∞—Å–Ω–æ" ‚Üí 0.5 (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ)
- "–í –ø–µ—â–µ—Ä–µ –∑–∞—Å–∞–¥–∞ –∏–∑ 5 –≥–æ–±–ª–∏–Ω–æ–≤" ‚Üí 0.0 (GM secret, –Ω–µ –∑–Ω–∞–µ—Ç)
"""
```

**Retrieval filtering:**

```python
# –î–ª—è player-facing responses
memories = await search_memories(
    query=query,
    min_confidence=0.5  # –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–∂ –∑–Ω–∞–µ—Ç/–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç
)

# –î–ª—è GM narrative generation
all_memories = await search_memories(
    query=query,
    min_confidence=0.0  # –í—Å—ë, –≤–∫–ª—é—á–∞—è secrets
)
```

**Fallback –µ—Å–ª–∏ LLM –æ—à–∏–±—Å—è:**

```python
# Conservative default
confidence = parsed_json.get("player_knowledge_confidence", 1.0)

# Assume player knows by default (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ —á–µ–º —Å–ø–æ–π–ª–µ—Ä—ã)
```

**Benefits:**
- Soft filtering - –µ—Å–ª–∏ LLM –æ—à–∏–±—Å—è —Å 0.9 –≤–º–µ—Å—Ç–æ 1.0, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
- Temporal ranking —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç –≤–∞–∂–Ω–æ—Å—Ç—å
- –ù–µ—Ç double-check LLM calls

---

## Week 1: Database Setup

### Task 1.1: Updated Database Schema

**File:** `app/db/migrations/001_initial_schema.sql`

**Key changes:**
- ‚úÖ Add `player_knowledge_confidence` column
- ‚ùå Remove `summary` from game_sessions
- ‚úÖ Add indexes for temporal ranking

```sql
-- ============================================
-- TABLE: episodic_memories (UPDATED)
-- ============================================
CREATE TABLE episodic_memories (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    character_id UUID NOT NULL REFERENCES characters(id) ON DELETE CASCADE,
    session_id UUID REFERENCES game_sessions(id) ON DELETE SET NULL,
    
    content TEXT NOT NULL,
    embedding VECTOR(1536),
    
    memory_type VARCHAR(50) DEFAULT 'event',
    importance_score INT DEFAULT 5 CHECK (importance_score >= 0 AND importance_score <= 10),
    
    -- üÜï Knowledge scoping –¥–ª—è –º–µ—Ç–∞–≥–µ–π–º–∏–Ω–≥ prevention
    player_knowledge_confidence FLOAT DEFAULT 1.0 
        CHECK (player_knowledge_confidence >= 0.0 AND player_knowledge_confidence <= 1.0),
    
    entities TEXT[],
    location VARCHAR(200),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT fk_character FOREIGN KEY (character_id) REFERENCES characters(id),
    CONSTRAINT fk_session FOREIGN KEY (session_id) REFERENCES game_sessions(id)
);

-- Indexes –¥–ª—è layered retrieval
CREATE INDEX idx_memories_character_id ON episodic_memories(character_id);
CREATE INDEX idx_memories_created_at ON episodic_memories(created_at DESC);
CREATE INDEX idx_memories_importance ON episodic_memories(importance_score DESC);
CREATE INDEX idx_memories_confidence ON episodic_memories(player_knowledge_confidence DESC);

-- Vector index
CREATE INDEX idx_memories_embedding ON episodic_memories 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- ============================================
-- TABLE: game_sessions (SIMPLIFIED)
-- ============================================
CREATE TABLE game_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    character_id UUID NOT NULL REFERENCES characters(id) ON DELETE CASCADE,
    
    started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    ended_at TIMESTAMP WITH TIME ZONE,
    
    -- ‚ùå summary —É–¥–∞–ª–µ–Ω - –∏–∑–±—ã—Ç–æ—á–µ–Ω –ø—Ä–∏ layered retrieval
    
    turns_count INT DEFAULT 0,
    total_damage_dealt INT DEFAULT 0,
    total_damage_taken INT DEFAULT 0,
    
    CONSTRAINT fk_character FOREIGN KEY (character_id) REFERENCES characters(id)
);
```

---

## Week 2: Memory System Implementation

### Task 2.1: Layered Memory Retrieval

**File:** `app/memory/retrieval.py`

```python
"""Optimized memory retrieval with layered approach."""
from typing import List, Dict
from uuid import UUID
import asyncpg
from app.config import settings
from app.memory.embeddings import embeddings_service
import logging

logger = logging.getLogger(__name__)


class MemoryRetrieval:
    """Layered memory retrieval system."""
    
    def __init__(self):
        self.db_url = settings.SUPABASE_DB_URL
    
    async def get_context_for_turn(
        self,
        character_id: UUID,
        query: str,
        scope: str = "player"  # "player" or "gm"
    ) -> List[Dict]:
        """
        Get memory context using layered retrieval.
        
        Layers:
        1. Recent (last 10 memories)
        2. Important (importance >= 7, any age)
        3. Semantic (vector search, top 5)
        
        Args:
            character_id: UUID of character
            query: Current action/query
            scope: "player" (filter GM secrets) or "gm" (all memories)
            
        Returns:
            List of memories, deduplicated and sorted by relevance
        """
        
        # Step 1: Get recent memories (cheap SQL)
        recent = await self._get_recent_layer(character_id, limit=10, scope=scope)
        
        # Step 2: Get important + semantic in one pass
        important_and_semantic = await self._get_important_semantic_layer(
            character_id=character_id,
            query=query,
            top_k=5,
            min_importance=6,
            recency_weight=0.3,
            scope=scope
        )
        
        # Step 3: Deduplicate and combine
        all_memories = self._deduplicate(recent + important_and_semantic)
        
        logger.info(f"Retrieved {len(all_memories)} memories for context")
        
        return all_memories
    
    async def _get_recent_layer(
        self,
        character_id: UUID,
        limit: int,
        scope: str
    ) -> List[Dict]:
        """Layer 1: Recent memories."""
        conn = await asyncpg.connect(self.db_url)
        
        try:
            # Confidence filter based on scope
            min_confidence = 0.5 if scope == "player" else 0.0
            
            sql = """
                SELECT id, content, memory_type, importance_score,
                       player_knowledge_confidence, entities, location, created_at
                FROM episodic_memories
                WHERE character_id = $1
                  AND player_knowledge_confidence >= $2
                ORDER BY created_at DESC
                LIMIT $3
            """
            
            rows = await conn.fetch(sql, character_id, min_confidence, limit)
            return [dict(row) for row in rows]
            
        finally:
            await conn.close()
    
    async def _get_important_semantic_layer(
        self,
        character_id: UUID,
        query: str,
        top_k: int,
        min_importance: int,
        recency_weight: float,
        scope: str
    ) -> List[Dict]:
        """
        Layer 2+3: Important events + Semantic search with temporal ranking.
        
        Combined in one query for efficiency.
        """
        
        # Generate query embedding
        query_embedding = await embeddings_service.embed_text(query)
        
        conn = await asyncpg.connect(self.db_url)
        
        try:
            min_confidence = 0.5 if scope == "player" else 0.0
            
            sql = """
                SELECT 
                    id, content, memory_type, importance_score,
                    player_knowledge_confidence, entities, location, created_at,
                    
                    -- Semantic similarity
                    1 - (embedding <=> $2::vector) as semantic_similarity,
                    
                    -- Recency score (1.0 = today, 0.0 = 30+ days ago)
                    GREATEST(0, 1 - (EXTRACT(EPOCH FROM (NOW() - created_at)) / 86400.0 / 30)) as recency_score,
                    
                    -- Combined score
                    (1 - (embedding <=> $2::vector)) * $4 + 
                    GREATEST(0, 1 - (EXTRACT(EPOCH FROM (NOW() - created_at)) / 86400.0 / 30)) * $5
                    as final_score
                    
                FROM episodic_memories
                WHERE character_id = $1
                  AND importance_score >= $3
                  AND player_knowledge_confidence >= $6
                ORDER BY final_score DESC
                LIMIT $7
            """
            
            rows = await conn.fetch(
                sql,
                character_id,
                query_embedding,
                min_importance,
                1.0 - recency_weight,  # semantic weight
                recency_weight,         # recency weight
                min_confidence,
                top_k
            )
            
            return [dict(row) for row in rows]
            
        finally:
            await conn.close()
    
    def _deduplicate(self, memories: List[Dict]) -> List[Dict]:
        """Remove duplicates by ID, keep first occurrence."""
        seen = set()
        unique = []
        
        for mem in memories:
            if mem['id'] not in seen:
                seen.add(mem['id'])
                unique.append(mem)
        
        return unique


# Global instance
memory_retrieval = MemoryRetrieval()
```

---

### Task 2.2: Smart Memory Creation

**File:** `app/memory/smart_storage.py`

```python
"""Smart memory storage with LLM-based filtering."""
from typing import Optional, Dict
from uuid import UUID
from app.memory.episodic import episodic_memory
import logging

logger = logging.getLogger(__name__)


async def create_smart_memory(
    character_id: UUID,
    session_id: Optional[UUID],
    user_action: str,
    gm_response: str,
    metadata: Dict,
    location: Optional[str] = None
) -> Optional[UUID]:
    """
    Create memory with smart filtering based on Synthesizer metadata.
    
    Args:
        character_id: Character UUID
        session_id: Session UUID
        user_action: Player's action
        gm_response: GM's response
        metadata: Metadata from Response Synthesizer:
            {
                "importance_score": 0-10,
                "player_knowledge_confidence": 0.0-1.0,
                "key_entities": ["entity1", "entity2"],
                "memory_type": "event|dialogue|discovery|combat"
            }
        location: Current location
        
    Returns:
        Created memory UUID or None if filtered out
    """
    
    importance = metadata.get("importance_score", 5)
    confidence = metadata.get("player_knowledge_confidence", 1.0)
    entities = metadata.get("key_entities", [])
    memory_type = metadata.get("memory_type", "event")
    
    # Filter: Don't store trivial memories
    if importance < 3:
        logger.info(f"Skipping low-importance memory (score={importance})")
        return None
    
    # Build content
    content = f"Player: {user_action}\nGM: {gm_response[:200]}"
    
    # Create memory
    memory_id = await episodic_memory.create_memory(
        character_id=character_id,
        session_id=session_id,
        content=content,
        memory_type=memory_type,
        importance_score=importance,
        player_knowledge_confidence=confidence,
        entities=entities,
        location=location
    )
    
    logger.info(
        f"Created memory {memory_id}: importance={importance}, "
        f"confidence={confidence}, type={memory_type}"
    )
    
    return memory_id
```

---

### Task 2.3: Updated Response Synthesizer

**File:** `app/agents/response_synthesizer.py` (UPDATE)

Add metadata extraction to existing prompt:

```python
class ResponseSynthesizerAgent(BaseAgent):
    """..."""
    
    def __init__(self):
        super().__init__(
            name="ResponseSynthesizer",
            model="gpt-4o",
            temperature=0.3
        )
    
    async def execute(self, context: dict[str, Any]) -> dict[str, Any]:
        """..."""
        
        system_prompt = """–¢—ã ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ GM.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å mechanics result –∏ narrative description
2. –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ —Å —ç–º–æ–¥–∑–∏ –∏ Markdown
3. üÜï –î–û–ë–ê–í–ò–¢–¨ METADATA –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏

–í–µ—Ä–Ω–∏ JSON:
{
  "final_message": "formatted message",
  "memory_metadata": {
    "importance_score": 0-10,
    "player_knowledge_confidence": 0.0-1.0,
    "key_entities": ["entity names"],
    "memory_type": "event|dialogue|discovery|combat"
  }
}

–ö–†–ò–¢–ï–†–ò–ò IMPORTANCE:
- 9-10: Boss fights, plot twists, character death, major quest milestones
- 6-8: Quest start/end, NPC betrayals, significant combat, major discoveries
- 3-5: Normal combat, dialogue with NPCs, exploration, minor loot
- 0-2: Trivial actions (–æ—Å–º–æ—Ç—Ä–µ—Ç—å—Å—è, –ø–æ–π—Ç–∏ –≤–ø–µ—Ä–µ–¥), simple observations

–ö–†–ò–¢–ï–†–ò–ò CONFIDENCE:
- 1.0: –ü–µ—Ä—Å–æ–Ω–∞–∂ observed/heard directly, told by NPC, –æ–±—â–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã
- 0.5: Rumor, inference, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –Ω–µ—è–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- 0.0: GM secret, –ø–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ –º–æ–∂–µ—Ç –∑–Ω–∞—Ç—å (–∑–∞—Å–∞–¥—ã, –ø–ª–∞–Ω—ã –≤—Ä–∞–≥–æ–≤, –±—É–¥—É—â–µ–µ)

KEY_ENTITIES: –ò–∑–≤–ª–µ–∫–∏ –∏–º–µ–Ω–∞ NPC, –ª–æ–∫–∞—Ü–∏–∏, –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã –∏–∑ action/response
"""
        
        # ... (existing execution logic)
        
        # Parse response
        try:
            response_data = json.loads(llm_response)
            
            # Validate metadata structure
            if "memory_metadata" not in response_data:
                # Fallback to safe defaults
                response_data["memory_metadata"] = {
                    "importance_score": 5,
                    "player_knowledge_confidence": 1.0,
                    "key_entities": [],
                    "memory_type": "event"
                }
        except json.JSONDecodeError:
            logger.error("Failed to parse Synthesizer JSON, using defaults")
            response_data = {
                "final_message": llm_response,
                "memory_metadata": {
                    "importance_score": 5,
                    "player_knowledge_confidence": 1.0,
                    "key_entities": [],
                    "memory_type": "event"
                }
            }
        
        return response_data
```

---

## Week 3: Integration & Testing

### Task 3.1: Update Bot Handlers

**File:** `app/bot/handlers.py` (UPDATE)

```python
from app.memory.retrieval import memory_retrieval
from app.memory.smart_storage import create_smart_memory

@router.message(ConversationState.in_conversation, F.text)
async def handle_conversation(message: Message, state: FSMContext):
    """Main conversation handler with optimized memory."""
    
    user_message = message.text
    telegram_user_id = message.from_user.id
    
    # Load character from DB
    character = await character_repo.get_character_by_telegram_id(telegram_user_id)
    
    if not character:
        await message.answer("‚ùå –ü–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π /start")
        return
    
    # Get world state
    game_state = await world_state_agent.load_world_state(character.id)
    
    # Get/create session
    data = await state.get_data()
    session_id = data.get("session_id")
    
    if not session_id:
        session_id = await session_manager.start_session(character.id)
        await state.update_data(session_id=session_id)
    
    # üÜï Layered memory retrieval
    memories = await memory_retrieval.get_context_for_turn(
        character_id=character.id,
        query=user_message,
        scope="player"  # Filter GM secrets
    )
    
    # Typing indicator
    typing_task = asyncio.create_task(_send_typing_indicator(message))
    
    try:
        # Process through orchestrator
        final_message, updated_character, updated_game_state, metadata = \
            await orchestrator.process_action(
                user_action=user_message,
                character=character,
                game_state=game_state,
                memories=memories
            )
    finally:
        typing_task.cancel()
    
    # Save character to DB
    await character_repo.create_or_update_character(updated_character)
    
    # üÜï Smart memory creation (zero overhead - metadata from Synthesizer)
    await create_smart_memory(
        character_id=updated_character.id,
        session_id=session_id,
        user_action=user_message,
        gm_response=final_message,
        metadata=metadata,  # From Synthesizer JSON
        location=updated_game_state.get("location")
    )
    
    # Send response
    await message.answer(final_message, parse_mode="Markdown")
```

---

## Cost Analysis (Updated)

### Per-Turn Cost Breakdown:

```
Rules Arbiter:      gpt-4o-mini  ~200 tokens  = $0.00003
Narrative Director: grok-2       ~800 tokens  = $0.00120
Response Synthesizer: gpt-4o     ~600 tokens  = $0.00090  (+100 tokens –¥–ª—è metadata)
World State:        gpt-4o-mini  ~150 tokens  = $0.00002
Memory Retrieval:   embeddings   ~50 tokens   = $0.00000  (negligible)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:                                         $0.00215 (~‚ÇΩ0.22)
```

**Overhead –æ—Ç Sprint 2:** +$0.00025 (12%)

**Savings vs original plan:**
- ‚ùå No session summary generation: -$0.001
- ‚ùå No separate importance scorer: -$0.00002
- ‚ùå No confidence validation: -$0.00002
- ‚ùå No CrewAI overhead: -$0 (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º)

**Net overhead:** –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ù–û–õ–¨ (metadata –±–µ—Å–ø–ª–∞—Ç–Ω–∞ –≤ existing Synthesizer call)

---

## Success Criteria Checklist

–ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ Sprint 3 –ø—Ä–æ–≤–µ—Ä—å:

- [ ] **Database:**
  - [ ] Supabase project —Å–æ–∑–¥–∞–Ω
  - [ ] Migration 001 –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ (—Å player_knowledge_confidence)
  - [ ] pgvector extension —Ä–∞–±–æ—Ç–∞–µ—Ç
  
- [ ] **Memory System:**
  - [ ] Layered retrieval (recent + important + semantic) —Ä–∞–±–æ—Ç–∞–µ—Ç
  - [ ] Temporal ranking –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ recency
  - [ ] Latency <500ms –¥–ª—è retrieval
  - [ ] Accuracy >85% (subjective evaluation)
  
- [ ] **Smart Storage:**
  - [ ] Importance scoring —á–µ—Ä–µ–∑ Synthesizer metadata
  - [ ] Confidence scores –ø—Ä–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
  - [ ] Trivial memories (importance <3) —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è
  
- [ ] **Knowledge Scoping:**
  - [ ] GM secrets (confidence=0.0) –Ω–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ player retrieval
  - [ ] Player-facing responses –Ω–µ —Å–ø–æ–π–ª–µ—Ä—è—Ç
  
- [ ] **Persistence:**
  - [ ] Characters —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ DB
  - [ ] Sessions tracking —Ä–∞–±–æ—Ç–∞–µ—Ç
  - [ ] Memories —Å–æ–∑–¥–∞—é—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ö–æ–¥–∞
  
- [ ] **Multi-session:**
  - [ ] –ë–æ—Ç –ø–æ–º–Ω–∏—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ –ø—Ä–æ—à–ª—ã—Ö —Å–µ—Å—Å–∏–π
  - [ ] Temporal ranking —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
  - [ ] –ù–µ—Ç —è–≤–Ω—ã—Ö session boundaries –¥–ª—è –∏–≥—Ä–æ–∫–∞

---

## Migration from SPRINT3_SPEC.md

**Files to SKIP/DELETE:**
- ‚ùå `app/agents/crew_config.py` - CrewAI –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ MVP
- ‚ùå `app/agents/crew_orchestrator.py` - Simple orchestrator –≤–º–µ—Å—Ç–æ CrewAI
- ‚ùå `app/db/sessions.py ‚Üí generate_session_summary()` - –ù–µ –Ω—É–∂–µ–Ω
- ‚ùå `app/memory/chunking.py` - –£–ø—Ä–æ—â–µ–Ω–æ –≤ layered retrieval

**Files to UPDATE:**
- ‚úÖ `app/db/migrations/001_initial_schema.sql` - Add player_knowledge_confidence, remove summary
- ‚úÖ `app/db/models.py` - Update EpisodicMemoryDB, GameSessionDB
- ‚úÖ `app/agents/response_synthesizer.py` - Add metadata extraction
- ‚úÖ `app/memory/episodic.py` - Add confidence filtering
- ‚úÖ `app/bot/handlers.py` - Use layered retrieval + smart storage

**New Files:**
- ‚úÖ `app/memory/retrieval.py` - Layered memory retrieval
- ‚úÖ `app/memory/smart_storage.py` - Smart memory creation

---

## Next Steps –¥–ª—è PM

1. **Review —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç** –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ SPRINT3_SPEC.md
2. **–î–∞—Ç—å AI agent –∑–∞–¥–∞—á—É:**
   ```
   "Implement Sprint 3 according to SPRINT3_UPDATED.md.
   Start with Task 1.1: Database schema migration with updated fields."
   ```
3. **Testing workflow:**
   - –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –Ω–µ–¥–µ–ª–∏ –¥–µ–ª–∞—Ç—å integration test
   - –ü—Ä–æ–≤–µ—Ä—è—Ç—å cost per turn (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ~$0.0022)
   - Subjective evaluation: –ø–æ–º–Ω–∏—Ç –ª–∏ –±–æ—Ç past events?

---

## Future Enhancements (Post-Sprint 3)

**Sprint 4 –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- **NPC relationship tracking** –≤ structured format (–≤–º–µ—Å—Ç–æ episodic memories)
- **Quest system** —á–µ—Ä–µ–∑ world_state —Å flexible outcomes
- **Memory consolidation** –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π (100+ sessions)
- **CrewAI migration** –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è parallel execution

**Pricing model:**
- –ü–æ–¥–ø–∏—Å–∫–∞: ‚ÇΩ500/–º–µ—Å (unlimited turns, fair use ~100/–¥–µ–Ω—å)
- Cost per user per month: ‚ÇΩ66 (300 turns √ó ‚ÇΩ0.22)
- Margin: 87% (–æ—Ç–ª–∏—á–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞)

---

**–í–æ–ø—Ä–æ—Å—ã?** –ü–∏—à–∏ –≤ issues.

**Ready to implement?** –ü–æ–≥–Ω–∞–ª–∏! üöÄ
